{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sec_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNDQXeB9Tb04WADG6d+WJN2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reckoning-machines/sec_test/blob/master/sec_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxREunso0REA",
        "colab_type": "code",
        "outputId": "40e5ed9f-4445-442b-9327-f280f19e33ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# why use R here?  \n",
        "# edgarWebR pulls sections really well\n",
        "\n",
        "!git clone https://github.com/reckoning-machines/sec_test.git #errors if exists but doesn't halt exe\n",
        "!cp \"sec_test/test_ticker_list.csv\" \"test_ticker_list.csv\" #move ticker list to root\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'sec_test' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04jdag6BhSmY",
        "colab_type": "code",
        "outputId": "ee9ae820-277a-4811-9480-720ef63f7f95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# activate R magic\n",
        "import rpy2\n",
        "%load_ext rpy2.ipython"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:14: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
            "  from pandas.core.index import Index as PandasIndex\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:34: UserWarning: pandas >= 1.0 is not supported.\n",
            "  warnings.warn('pandas >= 1.0 is not supported.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2x4-h6m0x-V",
        "colab_type": "code",
        "outputId": "e7298b65-7870-49fe-f6d5-73ed21ae7e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%R\n",
        "\n",
        "#installs take a bit of time...\n",
        "devtools::install_github(\"mwaldstein/edgarWebR\")\n",
        "devtools::install_github(\"r-lib/xml2\") #this for edgarWebR \n",
        "#devtools::install_github(\"DavisVaughan/furrr\")\n",
        "install.packages('furrr')\n",
        "library(edgarWebR) #this is an up to date library with an active maintainer.\n",
        "library(xml2)\n",
        "library(knitr)\n",
        "library(dplyr)\n",
        "library(purrr)\n",
        "library(rvest)\n",
        "library(tidyr)\n",
        "library(readr)\n",
        "#library(textclean) #not using this yet.\n",
        "library(furrr) #multiprocessing... does colab use it\n",
        "\n",
        "LOGFILE = format(Sys.time(), \"%b_%d_%Y.log\")\n",
        "print(LOGFILE)\n",
        "\n",
        "CSVFILE = format(Sys.time(), \"%b_%d_%Y.csv\")\n",
        "print(CSVFILE)\n",
        "\n",
        "get_filings_links <-function(str_ticker) {\n",
        "    df_filings <- company_filings(str_ticker, type = \"10-\", count = 20)\n",
        "    df_filings <- df_filings[df_filings$type == \"10-K\" | df_filings$type == \"10-Q\", ]\n",
        "    df_filing_infos <- map_df(df_filings$href, filing_information)\n",
        "    df_filings <- bind_cols(df_filings, df_filing_infos)\n",
        "    return(head(as_tibble(df_filings),20))\n",
        "  }\n",
        "\n",
        "write_log <- function(str_text) {\n",
        "      print(str_text)\n",
        "      if (file.exists(LOGFILE)) {\n",
        "          write(str_text,file=LOGFILE,append=TRUE)\n",
        "      } else {\n",
        "          write(str_text,file=LOGFILE,append=FALSE)\n",
        "      }\n",
        "\n",
        "  }\n",
        "\n",
        "write_log_csv <- function(df) {\n",
        "    if (file.exists(CSVFILE)) {\n",
        "          write_csv(df,CSVFILE,append=TRUE)\n",
        "      } else {\n",
        "          write_csv(df,CSVFILE,append=FALSE)\n",
        "      }\n",
        "\n",
        "  }\n",
        "\n",
        "get_mdna_text <- function(str_href) {\n",
        "  write_log(\"next link:\")\n",
        "  write_log(str_href)\n",
        "\n",
        "  #make this a func\n",
        "  str_file_path <- ''\n",
        "  file_path = strsplit(str_href,'/')\n",
        "  for (i in 5:length(file_path[[1]])-1) {\n",
        "    str_file_path = paste0(str_file_path,\"/\",(file_path[[1]][i]))\n",
        "  }\n",
        "  str_file_path <- paste0(getwd(),\"/\",str_file_path)\n",
        "  dir.create(str_file_path,recursive = TRUE)\n",
        "  str_file_path\n",
        "  str_file_name <- ''\n",
        "  file_path = strsplit(str_href,'/')\n",
        "  for (i in 4:length(file_path[[1]])) {\n",
        "    str_file_name = paste0(str_file_name,\"/\",(file_path[[1]][i]))\n",
        "  }\n",
        "  str_file_name <- paste0(getwd(),str_file_name)\n",
        "  str_file_name <- gsub(\".htm\",\".csv\",str_file_name)\n",
        "  \n",
        "  str_section = 'item 2|item 7'\n",
        "  str_search = 'discussion'\n",
        "\n",
        "  if (file.exists(str_file_name)) {  #add force equals true\n",
        "    write_log(\"filing documents from cache ...\")\n",
        "    \n",
        "    df_filing_documents <- read_csv(str_file_name,col_types = cols()) \n",
        "    df_filing_documents <- df_filing_documents %>% mutate_if(is.logical, as.character)\n",
        "  } else {\n",
        "    write_log(\"filing documents from sec ...\")\n",
        "    \n",
        "    df_filing_documents <- filing_documents(str_href) %>%\n",
        "      filter(!grepl('.pdf',href)) %>%\n",
        "      write_csv(str_file_name)\n",
        "  }\n",
        "  \n",
        "  str_doc_href <- df_filing_documents[df_filing_documents$type == \"10-K\" | df_filing_documents$type == \"10-Q\",]$href\n",
        "  \n",
        "  print(df_filing_documents[df_filing_documents$type == \"10-K\" | df_filing_documents$type == \"10-Q\",])  \n",
        "  \n",
        "  file_end <- gsub(\"https://www.sec.gov\",'',str_doc_href)\n",
        "  \n",
        "  file_name = paste0(getwd(),file_end)\n",
        "  \n",
        "  #use cache if possible\n",
        "  if (file.exists(file_name)) {\n",
        "\n",
        "    doc <- read_csv(file_name,col_types = cols(.default = \"c\"))\n",
        "    print(\"local cache\")\n",
        "    \n",
        "  } else {\n",
        "\n",
        "    doc <- parse_filing(str_doc_href)    \n",
        "\n",
        "    str_file_path <- ''\n",
        "    file_path = strsplit(file_name,'/')\n",
        "    for (i in 3:length(file_path[[1]])-1) {\n",
        "      str_file_path = paste0(str_file_path,\"/\",(file_path[[1]][i]))\n",
        "    }\n",
        "    str_file_path <- paste0(str_file_path,\"/\")\n",
        "    dir.create(str_file_path,recursive = TRUE)\n",
        "    write_csv(as_tibble(doc),file_name)\n",
        "    \n",
        "  }\n",
        "\n",
        "  df_txt <- doc[grepl(str_section, doc$item.name, ignore.case = TRUE) & grepl(str_search, doc$item.name, ignore.case = TRUE), ] # only discussion for now\n",
        "  #if default search fails, use a dictionary attempt\n",
        "  if (nrow(df_txt) == 0) {\n",
        "    write_log('going to backup')\n",
        "    #paired vector of start and ending text to slice if found\n",
        "    #going forward use tickers as an additional column\n",
        "    #and port this to a csv file as part of the install.\n",
        "    df_filter_list <- data.frame(\n",
        "      start_text = c('Introduction',\n",
        "                     'FUNCTIONAL EARNINGS', \n",
        "                     'DISCUSSION AND ANALYSIS',\n",
        "                     'DISCUSSION AND ANALYSIS',\n",
        "                     'DISCUSSION AND ANALYSIS',\n",
        "                     'OVERVIEW',\n",
        "                     'Business Overview',\n",
        "                     'Financial Review',\n",
        "                     'RESULTS OF OPERATIONS',\n",
        "                     'Overview',\n",
        "                     'Entergy operates',\n",
        "                     \"MANAGEMENT\\'S FINANCIAL DISCUSSION\",\n",
        "                     'General',\n",
        "                     \"Management's Discussion\",\n",
        "                     'EXECUTIVE SUMMARY',\n",
        "                     'EXECUTIVE OVERVIEW',\n",
        "                     'EXECUTIVE OVERVIEW',\n",
        "                     'The following management discussion and analysis',\n",
        "                     'CURRENT ECONOMIC CONDITIONS',\n",
        "                     'Overview and Highlights',\n",
        "                     'Financial Review - Results of Operations'),\n",
        "      end_text = c('Quantitative and qualitative disclosures about market risk',\n",
        "                   \"MANAGEMENT\\'S REPORT\",\n",
        "                   'RISK FACTORS',\n",
        "                   'FIVE-YEAR PERFORMANCE GRAPH',\n",
        "                   'FINANCIAL STATEMENTS AND NOTES',\n",
        "                   'Risk management includes the identification',\n",
        "                   'Selected Loan Maturity Data',\n",
        "                   'Risk Management',\n",
        "                   'QUANTITATIVE AND QUALITATIVE DISCLOSURES',\n",
        "                   'Forward-Looking Statements',\n",
        "                   'New Accounting Pronouncements',\n",
        "                   'New Accounting Pronouncements',\n",
        "                   'Website information',\n",
        "                   'Risk Disclosures',\n",
        "                   'RISK FACTORS',\n",
        "                   'A summary of contractual obligations is included',\n",
        "                   'CONSOLIDATED RESULTS OF OPERATIONS',\n",
        "                   'NON-GAAP FINANCIAL MEASURES',\n",
        "                   'FORWARD-LOOKING STATEMENTS',\n",
        "                   'Critical Accounting Policies and Estimates',\n",
        "                   'Unregistered Sales of Equity Securities and Use of Proceeds')\n",
        "    )\n",
        "    \n",
        "    #this would be case sensitive\n",
        "    for (row in 1:nrow(df_filter_list)) { #should flip this to apply()\n",
        "\n",
        "      start_text <- df_filter_list[row, \"start_text\"]\n",
        "      end_text <- df_filter_list[row, \"end_text\"]\n",
        "\n",
        "      write_log(paste0('trying ',start_text))\n",
        "      write_log(paste0('to ',end_text))\n",
        "\n",
        "      i_start = as.integer(which(grepl(start_text, doc$text))) \n",
        "      if (length(i_start) > 1) { #handle table of contents duplicates\n",
        "        i_start = i_start[2]\n",
        "      }\n",
        "      i_end = as.integer(which(grepl(end_text, doc$text)))\n",
        "      if (length(i_end) > 1) {\n",
        "        i_end = i_end[2]\n",
        "      }\n",
        "\n",
        "      write_log(i_start)\n",
        "      write_log(i_end)\n",
        "\n",
        "      if (length(i_start) != 0 & length(i_end) != 0) {\n",
        "        #i_start = as.numeric(i_start)\n",
        "        #i_end = as.numeric(i_end)\n",
        "        if (i_start < i_end) {        \n",
        "            print(paste0('istart is:',i_start,' iend is:',i_end))\n",
        "            df_txt = doc[i_start:i_end,]\n",
        "            break\n",
        "        }\n",
        "      }\n",
        "\n",
        "    }\n",
        "    if (length(i_start) == 0 || length(i_end) == 0) {\n",
        "      write_log(\"missing section for:\")\n",
        "      write_log(str_href)\n",
        "    }\n",
        "\n",
        "  }\n",
        "  #we could do some text preprocessing here.\n",
        "\n",
        "  df_txt <- as_tibble(df_txt) %>%\n",
        "    #mutate(text = textclean::strip(text)) %>%\n",
        "    mutate(section = str_search)\n",
        "\n",
        "  return(df_txt)\n",
        "}\n",
        "\n",
        "get_document_text <- function(str_ticker, force = FALSE) { #not using force yet\n",
        "  start_time <- Sys.time()\n",
        "\n",
        "  write_log(str_ticker)\n",
        "\n",
        "  str_write_name <- paste0('sec_data_folder/',str_ticker)\n",
        "\n",
        "  write_log(\"get filings links ...\")\n",
        "\n",
        "  filings_csv <- paste0(str_write_name,\"_filings.csv\")\n",
        "  \n",
        "  if (file.exists(filings_csv)) {  #add force equals true\n",
        "    write_log(\"from cache ...\")\n",
        "    \n",
        "    df_filings <- read_csv(filings_csv,col_types = cols()) \n",
        "    df_filings <- df_filings %>% mutate_if(is.logical, as.character)\n",
        "    } else {\n",
        "    write_log(\"from sec ...\")\n",
        "      \n",
        "    df_filings <- get_filings_links(str_ticker) %>%\n",
        "      mutate(ticker = str_ticker) %>%\n",
        "      write_csv(filings_csv)\n",
        "    }\n",
        "\n",
        "  write_log_csv(df_filings)\n",
        "  \n",
        "#for debug\n",
        "  i_test = nrow(df_filings) #for some reason this won't evaulate inside the if statement\n",
        "  if (i_test == 0) {\n",
        "      return(NULL)\n",
        "  }\n",
        "\n",
        "  write_log(\"get section text ...\")\n",
        "\n",
        "  df_data <- (df_filings) %>%\n",
        "    rowwise() %>%\n",
        "    mutate(nest_discussion = map(.x = href, .f = get_mdna_text)) %>%\n",
        "    ungroup() %>%\n",
        "    group_by(period_date) %>%\n",
        "    arrange(desc(period_date))\n",
        "\n",
        "  #jenky - find a rowwise application\n",
        "  a <- df_data %>%\n",
        "    select(period_date,filing_date,type,form_name,documents,nest_discussion) %>%\n",
        "    unnest(nest_discussion)\n",
        "\n",
        "  write_log(\"write to local csv  ...\")\n",
        "  df_data <- a %>%\n",
        "    as_tibble() %>%\n",
        "    write_csv(paste0(str_write_name,\".csv\"))\n",
        "\n",
        "  end_time <- Sys.time()\n",
        "\n",
        "  write_log(end_time - start_time)\n",
        "\n",
        "  return(df_data)\n",
        "}\n",
        "\n",
        "df_tickers <- read_csv('test_ticker_list.csv',col_types = cols()) \n",
        "\n",
        "dir.create('sec_data_folder', showWarnings = FALSE)\n",
        "\n",
        "future::plan(multiprocess)\n",
        "\n",
        "df_data <- future_map_dfr(df_tickers$Symbol, get_document_text,.progress = TRUE) #takes a few minutes\n",
        "#df_data <- map_df(df_tickers$Symbol, get_document_text) # non parallel version\n",
        "print('done')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Downloading GitHub repo mwaldstein/edgarWebR@master\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "curl (4.2 -> 4.3) [CRAN]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Installing 1 packages: curl\n",
            "\n",
            "R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/curl_4.3.tar.gz'\n",
            "\n",
            "R[write to console]: Content type 'application/x-gzip'\n",
            "R[write to console]:  length 673779 bytes (657 KB)\n",
            "\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: \n",
            "\n",
            "R[write to console]: downloaded 657 KB\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UYcX52F7_pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#move from R to python\n",
        "\n",
        "!pip install pandarallel\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "#from transformers import pipeline\n",
        "#nlp = pipeline('sentiment-analysis')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk import tokenize\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "import time\n",
        "pd.options.mode.chained_assignment = None\n",
        "pandarallel.initialize()\n",
        "df_tickers = pd.read_csv('test_ticker_list.csv')\n",
        "\n",
        "LOGFILE = 'sec_nlp_beta.log'\n",
        "f = open(LOGFILE, \"w\")\n",
        "t = time.localtime()\n",
        "current_time = time.strftime(\"%H:%M:%S\", t)\n",
        "f.write(current_time+\": process started\")\n",
        "f.close()\n",
        "\n",
        "FIND_WORDS = ['covid',\n",
        "              'guidance',\n",
        "              'outlook']\n",
        "\n",
        "def check_if_list_found_in_text(text, words=[], return_offset=False, lower_text=True):\n",
        "    result = []\n",
        "    text = (\n",
        "        \" \"\n",
        "        + text.replace(\"_\", \" \")\n",
        "        .replace(\"-\", \" \")\n",
        "        .replace(\",\", \" \")\n",
        "        .replace(\";\", \" \")\n",
        "        .replace('\"', \" \")\n",
        "        .replace(\":\", \" \")\n",
        "        .replace(\".\", \" \")\n",
        "        + \" \"\n",
        "    )\n",
        "    if lower_text:\n",
        "        text = text.lower()\n",
        "    for word in words:\n",
        "        word = (\n",
        "            \" \"\n",
        "            + word.replace(\"_\", \" \")\n",
        "            .replace(\"-\", \" \")\n",
        "            .replace(\",\", \" \")\n",
        "            .replace(\";\", \" \")\n",
        "            .replace('\"', \" \")\n",
        "            .replace(\":\", \" \")\n",
        "            .replace(\".\", \" \")\n",
        "            + \" \"\n",
        "        )\n",
        "        if lower_text:\n",
        "            word = word.lower()\n",
        "        if word in text:\n",
        "            if return_offset:\n",
        "                offset = text.find(word)\n",
        "                # offset = offset if not offset else offset-1\n",
        "                result.append(offset)\n",
        "            else:\n",
        "                result.append(word.strip())\n",
        "    return result\n",
        "\n",
        "def filter_stopwords(sent):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(sent)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_sentence = []\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "def sentiment_from_text(sentence):\n",
        "  sentence = filter_stopwords(sentence)\n",
        "  list_found = check_if_list_found_in_text(sentence,FIND_WORDS)\n",
        "  num_found = len(list_found)\n",
        "\n",
        "  ss = sid.polarity_scores(sentence) #NLTK\n",
        "  df = pd.DataFrame.from_dict(ss,orient = \"index\").T\n",
        "  df['transformers_score'] = dict_transformers['score'] #tranformers\n",
        "  df['transformers_label'] = dict_transformers['label']\n",
        "  df['text'] = sentence\n",
        "  df['keywords_found'] = num_found\n",
        "  return pd.concat(dict_sentiment)\n",
        "\n",
        "def filter_stopwords(sent):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(sent)\n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "  filtered_sentence = []\n",
        "  for w in word_tokens:\n",
        "      if w not in stop_words:\n",
        "          filtered_sentence.append(w)\n",
        "  return ' '.join(filtered_sentence)\n",
        "\n",
        "def df_from_text(text):\n",
        "  sentence_list = tokenize.sent_tokenize(text)\n",
        "  sentence_list\n",
        "  sid = SentimentIntensityAnalyzer()\n",
        "  list_df = []\n",
        "  for sentence in sentence_list:\n",
        "      sentence = filter_stopwords(sentence)\n",
        "      list_found = check_if_list_found_in_text(sentence,FIND_WORDS)\n",
        "      num_found = len(list_found)\n",
        "#if using transformers...\n",
        "#nlp = pipeline('sentiment-analysis')\n",
        "#nlp('We are very happy to include pipeline into the transformers repository.')\n",
        "#>>> {'label': 'POSITIVE', 'score': 0.99893874}\n",
        "      ss = sid.polarity_scores(sentence)\n",
        "      df = pd.DataFrame.from_dict(ss,orient = \"index\").T\n",
        "      df['text'] = sentence\n",
        "      df['keywords_found'] = num_found\n",
        "      list_df.append(df)\n",
        "      return pd.concat(list_df)\n",
        "\n",
        "def py_write_log(str_text):\n",
        "    t = time.localtime()\n",
        "    current_time = time.strftime(\"%H:%M:%S\", t)\n",
        "    print(str_text)\n",
        "    f = open(LOGFILE, \"a\")\n",
        "    f.write(current_time+\": \"+str_text)\n",
        "    f.close()\n",
        "    return\n",
        "\n",
        "def func_sentiment(row):\n",
        "    df = df_from_text(row['text']) #neg neu pos compound text keywords_found\n",
        "    neu = df.iloc[0]['neu']\n",
        "    pos = df.iloc[0]['pos']\n",
        "    neg = df.iloc[0]['neg']\n",
        "    num_rows = 1\n",
        "    compound = df.iloc[0]['compound']\n",
        "    text = df.iloc[0]['text']\n",
        "    keywords_found = df.iloc[0]['keywords_found']\n",
        "    return pd.Series([row['ticker'],row['section'],row['type'],row['period_date'],neu,pos,neg,compound,keywords_found,text,num_rows])\n",
        "\n",
        "master_list_df = []\n",
        "list_tickers = df_tickers['Symbol']\n",
        "#list_tickers = ['MMM']\n",
        "\n",
        "for ticker in list_tickers:\n",
        "    py_write_log(\"working on...\"+ticker)\n",
        "    tic = time.perf_counter()\n",
        "\n",
        "    if path.exists(\"sec_data_folder/\"+ticker+\".csv\"):\n",
        "\n",
        "        df_text = pd.read_csv(\"sec_data_folder/\"+ticker+\".csv\")\n",
        "        if len(df_text) > 0:\n",
        "\n",
        "            df_text['ticker'] = ticker\n",
        "\n",
        "            df_discussion = df_text[df_text['section']=='discussion']\n",
        "\n",
        "            df_out = df_discussion.parallel_apply(func_sentiment, axis=1)\n",
        "            df_out.columns = ['ticker','section','type','period_date','neu','neg','pos','compound','keywords_found','text','num_rows']\n",
        "            #df_out.to_csv(\"test.csv\")\n",
        "            if len(df_out) > 0:\n",
        "\n",
        "                df_out = df_out.groupby(['ticker','period_date','type']).sum().reset_index()\n",
        "                df_out['neg'] = df_out['neg']/df_out['num_rows']\n",
        "                df_out['neu'] = df_out['neu']/df_out['num_rows']\n",
        "                df_out['pos'] = df_out['pos']/df_out['num_rows']\n",
        "                df_out['compound'] = df_out['compound']/df_out['num_rows']\n",
        "\n",
        "                df_error = df_out[df_out['compound'] == 0]\n",
        "                if not df_error.empty:\n",
        "                    py_write_log(\"zero values...\"+ticker)\n",
        "                    df_error.to_csv('sec_nlp_errors.csv',mode = 'a')\n",
        "\n",
        "                df_out.drop(['keywords_found'],axis = 1)\n",
        "                df_out['compound_baseline'] = df_out['compound'] / df_out['compound'].mean()\n",
        "                df_out['neg_baseline'] = df_out['neg'] / df_out['neg'].mean()\n",
        "                df_out['pos_baseline'] = df_out['pos'] / df_out['pos'].mean()\n",
        "                df_out['compound_bdiff'] = df_out['compound_baseline'].diff()\n",
        "                df_out['neg_bdiff'] = df_out['neg_baseline'].diff()\n",
        "                df_out['pos_bdiff'] = df_out['pos_baseline'].diff()\n",
        "                df_out['compound_zscore'] = (df_out['compound'] - df_out['compound'].mean())/df_out['compound'].std(ddof=0)\n",
        "\n",
        "                #always cache\n",
        "                str_score_file = \"sec_data_folder/\"+ticker+\"_score.csv\"\n",
        "                df_out.to_csv(str_score_file)\n",
        "\n",
        "                master_list_df.append(df_out)\n",
        "            else:\n",
        "                py_write_log(\"missing...\"+ticker)\n",
        "        else:\n",
        "            py_write_log(ticker+\" has no data.\")\n",
        "    toc = time.perf_counter()\n",
        "    py_write_log(f\"Text processed in {toc - tic:0.4f} seconds\")\n",
        "\n",
        "if master_list_df:\n",
        "    df_data = pd.concat(master_list_df)\n",
        "    df_data.to_csv('df_data.csv')\n",
        "\n",
        "df = df_data[['period_date','ticker','compound_baseline']]\n",
        "df['quarter_end'] = pd.to_datetime(df['period_date'])\n",
        "df['quarter_end'] = df.quarter_end.map(lambda x: x.strftime('%Y-%m-%d'))\n",
        "\n",
        "#modify odd quarter ends\n",
        "df.loc[df.quarter_end == '2017-04-01', 'quarter_end'] = '2017-03-31'\n",
        "df.loc[df.quarter_end == '2017-07-01', 'quarter_end'] = '2017-06-30'\n",
        "df.loc[df.quarter_end == '2018-04-01', 'quarter_end'] = '2018-03-31'\n",
        "df.loc[df.quarter_end == '2018-07-01', 'quarter_end'] = '2018-06-30'\n",
        "\n",
        "df['quarter_end'] = pd.to_datetime(df['quarter_end'])\n",
        "df['quarter_end'] = df['quarter_end'].dt.to_period('q').dt.end_time #floor at end of quarter\n",
        "df['quarter_end'] = df.quarter_end.map(lambda x: x.strftime('%Y-%m-%d')) #format\n",
        "\n",
        "import numpy as np\n",
        "df_data_pivot = pd.pivot_table(df, values='compound_baseline', index=['quarter_end'],\n",
        "                columns=['ticker'], aggfunc=np.sum, fill_value=0).reset_index()\n",
        "\n",
        "df_data_pivot.to_csv(\"df_data_pivot.csv\")\n",
        "\n",
        "print('done!')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuyztVzxz7uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#post pivot data to Google Sheet\n",
        "\n",
        "!pip install --upgrade -q gspread\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials as GC\n",
        "gc = gspread.authorize(GC.get_application_default())\n",
        "# create, and save df\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "title = 'sec_nlp_data'\n",
        "gc.create(title)  # if not exist\n",
        "sheet = gc.open(title).sheet1\n",
        "set_with_dataframe(sheet, df_data_pivot) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcrhRus4oJJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.mode.chained_assignment = None\n",
        "df_plot = df_data[df_data['ticker'] == 'XOM'] #setting w copy warning\n",
        "\n",
        "df_plot['period_date'] = pd.to_datetime(df_plot['period_date'])\n",
        "df_plot['period_date'] = df_plot.period_date.map(lambda x: x.strftime('%Y-%m-%d')) #format\n",
        "\n",
        "df_plot.compound = pd.to_numeric(df_plot.compound)\n",
        "\n",
        "df_plot.plot.bar(x='period_date', y='compound', rot=90,title='XOM')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}